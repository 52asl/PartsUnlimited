##Scaling Redis##

Redis is a technology used to cache items from within your application, this making resource hungry requests less often and improving the read performance.

Azure Redis Cache is based on the open-source Redis cache. It gives you access to a secure, dedicated Redis cache, managed by Microsoft and accessible from any application within Azure. 

This section will see us reviewing common pitfalls with connection management, identifying an approach to keeping the items in our cache up to date and another to improve performance. We'll also look at what happens when the cache is failing over to a secondary or is offline how we detecting these failure conditions and ensure our application is resilient at failure time. 

###Cache options###

Redis is a quick access in memory data store, because of this there are several applications of how Redis can be integrated into your application. Including some which plug into existing ASP.NET extensibility points.

* Output cache - [Azure Redis ASP.Net Output cache provider](https://azure.microsoft.com/en-us/documentation/articles/cache-asp.net-output-cache-provider/)
* Session state - [Azure Redis Session State provider](https://azure.microsoft.com/en-us/documentation/articles/cache-asp.net-session-state-provider/)

The rest of this example will look at building out Redis within our application layer to store frequently accessed application data.

###Setup###

Redis in Azure comes in three service tiers Basic, Standard and Premium. 
Basic is good for testing but does not provide any replication or failover mechanisms and so is not recommended for production environments.

For getting a Redis cache setup, see [How to Use Azure Redis Cache](https://azure.microsoft.com/en-us/documentation/articles/cache-dotnet-how-to-use-azure-redis-cache/)

For more information regarding the tiers have a look at [Redis Tiers](https://azure.microsoft.com/en-us/pricing/details/cache/). 

###Client Libraries###

There are multiple client libraries across several languages. Each library can be used to connect into Redis servers. Because Parts unlimited is an ASP.NET application this example will use a C# library. The two most common and recommended by Redis.io client libraries for c# are ServiceStack.Redis and StackExchange.Redis.  For this example we have chosen to use StackExchange.Redis. 

See [here](http://redis.io/clients) for an up to date list of client libraries which support Redis.

After deciding on a client library, we must also determine which serialization library to use. A couple of contenders are [Json.Net](http://www.newtonsoft.com/json) and [Protobuf](https://code.google.com/p/protobuf-net/). In this example we will use Json.Net.

###Connection###

The first thing which is required is to setup a connection to the Redis cache which has been configured within Azure. The StackExchange.Redis client uses multiplexing through a single connection. The recommended usage is to create a single instance of the client and use this instance for all further commands issued to the cache. See the Parts Unlimited implementation of the connection [here](..\..\src\PartsUnlimitedWebsite\Cache\PartsUnlimitedRedisCache.cs) 

    private static Lazy<ConnectionMultiplexer> lazyConnection = new Lazy<ConnectionMultiplexer>(() =>
    {
        return ConnectionMultiplexer.Connect(configurationOptions);
    });

    public static ConnectionMultiplexer Connection
    {
        get { return lazyConnection.Value; } 
    }

> Because StackExchange.Redis uses multiplexing we should re-use the same instance of the ConnectionMultiplexer across the application. If you are wrapping up the Redis logic in one class ensure that this class is a singleton. Modern dependency injection frameworks have a way to achieve this.    

ConnectionMultiplexer implements `IDisposable` and is disposed when no longer required. We are explicitly not making use of a `using` statement. It's uncommon that you would use a `ConnectionMultiplexer` with a small life time, the idea is that it's reused. This is why we store it as a variable within a single instance of the `PartsUnlimitedRedisCache` class.  

###Query###

Querying data out of Redis is simple using the StackExchange.Redis libraries.
Below you can we retrieve a string based on a `key` and deserilize using the Json.net libraries into a typed `cacheitem`.  

    RedisValue redisValue = Connection.Database.StringGet(key);
    T cacheItem = JsonConvert.DeserializeObject<T>(redisValue);

Redis is not just a KVP storage mechanism, it supports other more complex data structures like lists, sorted and ordered sets. For more information see [An introduction to Redis data types and abstractions](http://redis.io/topics/data-types-intro).

###Store###

Storing data into Redis is equally simple using the StackExchange.Redis libraries.
Below we serialize an object using the Json.net libraries and store with a corresponding `key`.
The `span` attribute lets Redis know the expiration time / time to live (TTL) of the cache item.

	string stringValue= JsonConvert.SerializeObject(cacheItem, Formatting.Indented, settings);
    Connection.Database.StringSetAsync(key, stringValue, span, When.Always, commandFlags);

There are other controls when calling into `StringSetAsync` which define the TTL, replace and overwrite behaviors and fire and forget. For further explanation on the workings of the StackExchange.Redis libraries see [Basic Usage](https://github.com/StackExchange/StackExchange.Redis/blob/master/Docs/Basics.md)

Absolute expiration time is a native feature of Redis. To implement a sliding expiration we need to manage this ourselves. This can be achieved by resetting the TTL after each command. We have implemented this at query time and storing the SlidingCacheTime in the cache alongside the cached item. (So that when it is subsequently loaded it we can reset based on the original)

	CacheItem<T> cacheValue = JsonConvert.DeserializeObject<CacheItem<T>>(redisValue);
    if (cacheValue.SlidingCacheTime.HasValue)
    {
    	var timeSpan = cacheValue.SlidingCacheTime.Value;
	    await Database.KeyExpireAsync(key, timeSpan, cacheValue.Flags);
    } 

The setting of the expiration time a balance between being to short and too long. To short can cause you applications to continually retrieve data from the data store and add it to the cache. To long and the cached data is likely to become stale. 

Another consideration is the cost of the data retrieval. If you are calling a service which is expensive (time or cost) or the service is rate limited, then using Redis as an intermediary to buffer your calls could be an elegant solution here.

###Parts Unlimited Cache###

All commands which make use of caching in our business layer adhere to an interface. see [IPartsUnlimitedCache.cs](..\..\src\PartsUnlimitedWebsite\Cache\IPartsUnlimitedCache.cs). This interface defines common behavior and is not specific to any particular cache implementation. 

        Task SetValue<T>(string key, T value, PartsUnlimitedCacheOptions options);
        Task<CacheResult<T>> GetValue<T>(string key);
        Task Remove(string key);

By creating this contract and not directly using the underlying cache implementation in our code we are able to switch out the implementations of our cache with other implementations at run time. See SetupCache method in the [Startup.cs](..\..\src\PartsUnlimitedWebsite\Startup.cs) class where we switch between an in memory cache and Redis cache based on the existance of configuration.
	
> With all external dependencies its good practice to wrap the behavior behind an interface. This adheres to one of the Gang of Four design techniques - Program to an 'interface', not an 'implementation'. 
 
For Parts unlimited we have implemented the [Cache-Aside Pattern](https://msdn.microsoft.com/library/dn589799.aspx). 
This pattern loads data on demand into the cache from the underlying data store. Below is a snippet of this based from [CacheCoordinator.cs](..\..\src\PartsUnlimitedWebsite\CacheCoordinator.cs).

	//Try and load from the cache
	var result = await _cache.GetValue<T>(key);
	
	//Return cached item if it exists
	if (result.HasValue)
	{
		return result.Value;
	}

	//Load item from the underlying source and populate the cache.
	var sourceValue = await sourceLoader.Value;
	await _cache.SetValue(key, sourceValue, options.CacheOption);

Another approach to increase performance is to use Multi level caching. A Multi level cache works in conjunction with the Cache-Aside pattern, by storing two copies of the cached data. One copy in an external cache such as Redis and another copy an in memory cache internal to the application such as MemCache.  The flow here would be to have short expiration of the 1st level in memory cache and allow it to fall back to the 2nd level Redis cache. 
The benefits of this would mean that we avoid a network hop for frequently requested data.

> An in memory cache is private and so each instance of the application could potentially be maintaining inconsistent versions of the cached data. This becomes a prevalent when your site is backed by multiple Azure instances. To address this typically the 1st level in memory cache TTL would be less than the Redis cache TTL.

To implement this in Parts Unlimited we simply create another implementation of `IPartsUnlimitedCache.cs`. This implementation internally delegates the workload out out to the 1st and the 2nd level caches.

	public class PartUnlimitedMultilevelCache : IPartsUnlimitedCache
    {
       	....
        public async Task SetValue<T>(string key, T value, PartsUnlimitedCacheOptions options)
        {
            if (options.ShouldApplyToMultiLevelCache)
            {
                await _memoryCache.SetValue(key, value, options.SecondLevelCacheOptions);
            }

            await _redisCache.SetValue(key, value, options);
        }

        public async Task<CacheResult<T>> GetValue<T>(string key)
        {
            var memoryResult = await _memoryCache.GetValue<T>(key);
            if (memoryResult.HasValue)
            {
                return memoryResult;
            }

            return await _redisCache.GetValue<T>(key);
        }  
		....
	}

###Retry / Failover###

As developers we know transient errors occur and if not managed correctly will manifest into a poor experience for users of the application. 
To make our applications as robust and resilient as possible we need to consider patterns to reduce the impact of these transient errors. 

By using Azure and one of the standard or premium Azure Redis tiers you automatically have Replication and Failover built into the cache. While Azure is failing over to the secondary you potentially may see some failed requests.

####Minimising impact####

The StackExchange.Redis client has connection management retry logic built in to the libraries. This retry logic is only supported when establishing the initial connection to the cache and does not apply to operations and commands against the cache. The retry logic also does not have a configurable delay between retry attempts, simply it retries connecting after the connection timeout expires for the specified number of retries.

To have more control with our retry logic and also be able to apply this logic to more operations than just our initial connection you could use a Transient error handling framework, below are two available frameworks 

- [Transient fault handling application block](https://msdn.microsoft.com/en-us/library/dn440719.aspx)
- [Polly](http://www.hanselman.com/blog/NuGetPackageOfTheWeekPollyWannaFluentlyExpressTransientExceptionHandlingPoliciesInNET.aspx)

Unfortunately there is no direct support for Redis Cache in the either of these so we would need to customize the behavior to understand whether a Redis error is transient or not. 

To achieve this you could interrogate the internals of the `RedisConnectionException` to determine if it's a connection error which should be retried. This is based on the [ConnectionFailureType.FailureType](https://github.com/StackExchange/StackExchange.Redis/blob/master/StackExchange.Redis/StackExchange/Redis/ConnectionFailureType.cs), otherwise if it's a timeout we should also retry.

####Failure alternatives####

In the case where we have sustained repeated transient errors OR non transient errors  we need to ensure that our application is resilient when we cannot reach the underlying cache. One option here it to fallback to the source system. A business decision as to whether we redirect traffic to the source system or we notify the user that an error has occurred would need to be made. This decision should be based on the level of impact on your application or business would determine the decision here.

Expanding on our previous code snippet from [CacheCoordinator.cs](..\..\src\PartsUnlimitedWebsite\CacheCoordinator.cs) we can fallback to the source system when the cache key is not found within the cache. We are using the `options.CallFailOverOnError` to distinguish whether the item being requested should fallback to the source system.

    var result = await _cache.GetValue<T>(key);
    if (result.HasValue)
    {
        return result.Value;
    }

    //initial population.
    var sourceValue = await sourceLoader.Value;
    await _cache.SetValue(key, sourceValue, options.CacheOption);
    return sourceValue;

    //Cache has failed, fail back to source system.
    if (options.CallFailOverOnError || sourceLoader.IsValueCreated)
    {
        return await sourceLoader.Value;
	}

> If you relied on reading from the cache as part of the ordering process in an e-commerce store and this failed, this would potentially be a candidate to read from the underlying data store. Comparing this to a failure to read from the cache for display purposes, this would be less severe and perhaps acceptable to direct the user to an error page.

####Invalidate####

When items are put into a cache we generally specify a TTL (time to live) and whether this is a sliding or fixed period.

E.g. We cache product category which has a TTL of 10 minutes. 
After the 10 minutes expires Redis will evict this product category on our behalf. The next user who makes the same request will cause a cache miss, and will trigger a load from the source system to load the underlying record, push this into Redis and then return the category for the application. 

Ideally we would take the load time from the underlying data source away from the user to ensure they receive predictable performance. An approach to use here is to pre-load the cache in an out of proc process, this will not impact the user and ensure that the records in the cache are hot for end users to read.

> Consider priming the cache. This would be responsible for pre-populating commonly accessed items from the underlying source system into the cache. The Cache-Aside pattern would still be useful here for expired or evicted data. A implementation candidate could be a webjob running in the back ground either on a schedule or triggered by Azure storage / Azure queue. See [Create a .NET WebJob](https://azure.microsoft.com/en-us/documentation/articles/websites-dotnet-webjobs-sdk-get-started/)

####Operations / Monitoring####

**TODO : DS** Expand this section out

From within Azure portal you can review cache misses vs cache hits. 
For more information see [How to monitor Azure Redis Cache](https://azure.microsoft.com/en-us/documentation/articles/cache-how-to-monitor/#operations-and-alerts)

Alternatively for Azure cache monitoring outside the Azure portal using Microsoft Azure Insights SDK see  [Access Redis Cache Monitoring data](https://github.com/rustd/RedisSamples/tree/master/CustomMonitoring) 

Scaling through the portal [Scaling](https://azure.microsoft.com/en-us/documentation/articles/cache-how-to-scale/)

Scaling [How to automate a scaling operation](https://azure.microsoft.com/en-us/documentation/articles/cache-how-to-scale/#how-to-automate-a-scaling-operation)



