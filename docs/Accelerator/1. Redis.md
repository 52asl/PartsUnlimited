##Scaling Redis##

Redis is a technology used to cache items from within your application, this making resource hungry requests less often and improving the read performance.

Azure Redis Cache is based on the open-source Redis cache. It gives you access to a secure, dedicated Redis cache, managed by Microsoft and accessible from any application within Azure. 

In this section, we’ll review common pitfalls with connection management and identify approaches to keep the items in your cache up to date. We'll also look at what happens when the cache is failing over to a secondary - or is offline - and how we can detect these failure conditions to ensure the application is resilient during this period.  These suggestions will also address how to improve performance 

###Cache options###

Redis is a quick access in a remote memory data store, because of this there are several applications of how Redis can be integrated into your application. This includes some options that link into existing ASP.NET extensibility points.

* Output cache - [Azure Redis ASP.Net Output cache provider](https://azure.microsoft.com/en-us/documentation/articles/cache-asp.net-output-cache-provider/)
* Session state - [Azure Redis Session State provider](https://azure.microsoft.com/en-us/documentation/articles/cache-asp.net-session-state-provider/)

The majority of this example will look at building out Redis within our application layer to store frequently accessed application data.

###Setup###

Redis in Azure comes in three service tiers Basic, Standard and Premium. 
Basic is good for testing but does not provide any replication or failover mechanisms and therefore is not recommended for production environments.

For more information regarding the tiers, please refer to [Redis Tiers](https://azure.microsoft.com/en-us/pricing/details/cache/). 

For on setting up Redis cache in Azure, see [How to Use Azure Redis Cache](https://azure.microsoft.com/en-us/documentation/articles/cache-dotnet-how-to-use-azure-redis-cache/)

###Client Libraries###

There are multiple client libraries across several languages. Each library can be used to connect into Redis servers. Because Parts Unlimited is an ASP.NET application, this example will use a C# library. The two most common and recommended client libraries are ServiceStack.Redis and StackExchange.Redis.  For the purposes of this document, we have chosen to use StackExchange.Redis. 

See [here](http://redis.io/clients) for an up to date list of client libraries which support Redis.

After determining a client library, we must also determine which serialization library to use. A couple of contenders are [Json.Net](http://www.newtonsoft.com/json) and [Protobuf](https://code.google.com/p/protobuf-net/). In this example we will use Json.Net.

###Connection###

Once the Redis cache has been configured within Azure, the first task is to setup a connection. The StackExchange.Redis client uses multiplexing through a single connection. The recommended usage is to create a single instance of the client and use this instance for all further commands issued to the cache. See the Parts Unlimited implementation of the connection [here](..\..\src\PartsUnlimitedWebsite\Cache\PartsUnlimitedRedisCache.cs) 

    private static Lazy<ConnectionMultiplexer> lazyConnection = new Lazy<ConnectionMultiplexer>(() =>
    {
        return ConnectionMultiplexer.Connect(configurationOptions);
    });

    public static ConnectionMultiplexer Connection
    {
        get { return lazyConnection.Value; } 
    }

> Because StackExchange.Redis uses multiplexing we should re-use the same instance of the ConnectionMultiplexer across the application. If you are wrapping up the Redis logic in one class ensure that this class is a singleton. Modern dependency injection frameworks like Ninject and Unity have a way to achieve this at mapping time. To validate that multiple caches connections are not being created you can check the Azure portal Connection count metric.

ConnectionMultiplexer implements `IDisposable` and is disposed when no longer required. We are explicitly not making use of a `using` statement. It's uncommon that you would use a `ConnectionMultiplexer` with a small life time -the idea is that it's reused. This is why we store it as a variable within a single instance of the `PartsUnlimitedRedisCache` class.  

###Query###

Querying data out of Redis is simple using the StackExchange.Redis libraries.
Below we retrieve a string based on a `key` and deserialize using the Json.net libraries into a `cacheitem`.  

    RedisValue redisValue = Connection.Database.StringGet(key);
    T cacheItem = JsonConvert.DeserializeObject<T>(redisValue);

Redis is not just a KVP storage mechanism, it also supports other more complex data structures like sorted and ordered sets and lists. For more information see [An introduction to Redis data types and abstractions](http://redis.io/topics/data-types-intro).

###Store###

Storing data into Redis is equally simple using the StackExchange.Redis libraries.
Below we serialize an object using the Json.net libraries and store with a corresponding `key`.
The `span` attribute lets Redis know the expiration time / time to live (TTL) of the cache item.

	string stringValue= JsonConvert.SerializeObject(cacheItem, Formatting.Indented, settings);
    Connection.Database.StringSetAsync(key, stringValue, span, When.Always, commandFlags);

There are other controls when calling into `StringSetAsync` which define the TTL, replace and overwrite behaviours, and fire and forget. For further explanation on the workings of the StackExchange.Redis libraries. See [Basic Usage](https://github.com/StackExchange/StackExchange.Redis/blob/master/Docs/Basics.md).

Absolute expiration time is a native feature of Redis. To implement a sliding expiration we need to manage this ourselves. This can be achieved by resetting the TTL after each command. We have implemented this at query time by storing the SlidingCacheTime in the cache alongside the cached item. When the item is subsequently loaded we reset the TTL based on the original.

	CacheItem<T> cacheValue = JsonConvert.DeserializeObject<CacheItem<T>>(redisValue);
    if (cacheValue.SlidingCacheTime.HasValue)
    {
    	var timeSpan = cacheValue.SlidingCacheTime.Value;
	    await Database.KeyExpireAsync(key, timeSpan, cacheValue.Flags);
    } 

Guidance around setting of the expiration time depends on your application. It’s a balance between being too short and too long.  Too short of an expiration time can cause your applications to continually retrieve data from the data store and add it to the cache. Too long a time can result in the cached data becoming stale.

Another consideration is the cost of the data retrieval. If you are calling a service which is expensive (time or cost) or the service is rate limited, using Redis as an intermediary to buffer your calls could be an elegant solution.

###Parts Unlimited Cache###

All commands which make use of caching in our business layer adhere to an interface. See [IPartsUnlimitedCache.cs](..\..\src\PartsUnlimitedWebsite\Cache\IPartsUnlimitedCache.cs). This interface defines common behavior and is not specific to any particular cache implementation. 

        Task SetValue<T>(string key, T value, PartsUnlimitedCacheOptions options);
        Task<CacheResult<T>> GetValue<T>(string key);
        Task Remove(string key);

By creating this contract and not directly using the underlying cache constructs in our code we are able to switch out the implementations of our cache with other implementations at run time. See `SetupCache` method in the [Startup.cs](..\..\src\PartsUnlimitedWebsite\Startup.cs) class where we switch between an in memory cache and Redis cache based on the existence of Redis configuration.
	
> With all external dependencies it is a good practice to wrap the behavior behind an interface. This adheres to one of the Gang of Four design techniques - program to an 'interface', not an 'implementation'. 
 
For Parts Unlimited we have implemented the [Cache-Aside Pattern](https://msdn.microsoft.com/library/dn589799.aspx). This pattern loads data on demand into the cache from the underlying data store. Below is a snippet of this based from [CacheCoordinator.cs](..\..\src\PartsUnlimitedWebsite\Cache\CacheCoordinator.cs).

	//Try and load from the cache
	var result = await _cache.GetValue<T>(key);
	
	//Return cached item if it exists
	if (result.HasValue)
	{
		return result.Value;
	}

	//Load item from the underlying source and populate the cache.
	var sourceValue = await sourceLoader.Value;
	await _cache.SetValue(key, sourceValue, options.CacheOption);

Another approach to increase performance is to use multi-level caching. Multi-level cache works in conjunction with the Cache-Aside pattern by storing two copies of the cached data. This is accomplished by storing one copy in an external cache, such as, Redis, and another copy in an in-memory cache internal to the application, such as, MemCache. The flow has a short expiration of the first level in-memory cache and allows it to fall back to the second level Redis cache. The benefit of this is avoiding a network hop for frequently requested data.

An in-memory cache is private, so each instance of the application could potentially maintain inconsistent versions of the cached data. This becomes relevant when your site is backed by multiple Azure instances. To address this, typically the first level in-memory cache TTL would be less than the Redis cache TTL.

We have implemented multi-level cache in Parts Unlimited by simply creating another implementation of `IPartsUnlimitedCache.cs`. This implementation internally delegates the workload out to the first and the second level caches.

	public class PartUnlimitedMultilevelCache : IPartsUnlimitedCache
    {
       	....
        public async Task SetValue<T>(string key, T value, PartsUnlimitedCacheOptions options)
        {
            if (options.ShouldApplyToMultiLevelCache)
            {
                await _memoryCache.SetValue(key, value, options.SecondLevelCacheOptions);
            }

            await _redisCache.SetValue(key, value, options);
        }

        public async Task<CacheResult<T>> GetValue<T>(string key)
        {
            var memoryResult = await _memoryCache.GetValue<T>(key);
            if (memoryResult.HasValue)
            {
                return memoryResult;
            }

            return await _redisCache.GetValue<T>(key);
        }  
		....
	}

###Retry / Failover###

As developers, we know transient errors occur and if not managed correctly will manifest into a poor experience for users of the application. 
To make our applications as robust and resilient as possible, we need to consider patterns to reduce the impact of these transient errors. 

By using Azure and one of the standard or premium Azure Redis tiers, you automatically have replication and failover built into the cache. While Azure is failing over to the secondary you potentially may see some failed requests.

####Minimizing impact####

The StackExchange.Redis client has connection management retry logic built in to the libraries. This retry logic is only supported while establishing the initial connection to the cache and does not apply to operations and commands against the cache. The retry logic also does not have a configurable delay between retry attempts -simply it retries connecting after the connection timeout expires for the specified number of retries.

To have more control of our retry logic, and also be able to apply this logic to more operations than just the initial connection you could use a transient error handling framework. Below are two available frameworks :

- [Transient fault handling application block](https://msdn.microsoft.com/en-us/library/dn440719.aspx)
- [Polly](http://www.hanselman.com/blog/NuGetPackageOfTheWeekPollyWannaFluentlyExpressTransientExceptionHandlingPoliciesInNET.aspx)

There is no direct support for Redis Cache in the either of the two above framework. To understand whether a Redis error is transient or not, we would need to customize the behavior.

To achieve this, you could interrogate the internals of the `RedisConnectionException` to determine if it's a connection error which should be retried based on the [ConnectionFailureType.FailureType](https://github.com/StackExchange/StackExchange.Redis/blob/master/StackExchange.Redis/StackExchange/Redis/ConnectionFailureType.cs).  Otherwise, if the problem is a timeout, we should also retry.

####Failure alternatives####

In the case where we sustain repeated transient errors or non-transient errors, we need to ensure that our application is resilient when we cannot reach the underlying cache. One option here is to fall back to the source system. At this point, a decision needs to be made to redirect traffic to the source system or notify the user that an error has occurred. 

> If you relied on reading from the cache as part of the ordering process in an e-commerce store and this failed, this may be a candidate to read from the underlying data store. Comparing this to a failure to read from the cache for display purposes, this is less severe and perhaps acceptable to direct the user to an error page.

Expanding on our previous code snippet from [CacheCoordinator.cs](..\..\src\PartsUnlimitedWebsite\Cache\CacheCoordinator.cs) we can fallback to the source system when the cache key is not found. We are using the `options.CallFailOverOnError` to distinguish whether the item being requested should fall back to the source system or fail.

    var result = await _cache.GetValue<T>(key);
    if (result.HasValue)
    {
        return result.Value;
    }

    //initial population.
    var sourceValue = await sourceLoader.Value;
    await _cache.SetValue(key, sourceValue, options.CacheOption);
    return sourceValue;

    //Cache has failed, fail back to source system.
    if (options.CallFailOverOnError || sourceLoader.IsValueCreated)
    {
        return await sourceLoader.Value;
	}

	throw new InvalidOperationException($"Item in cache with key '{key}' not found");

####Invalidate####

When items are put into a cache we generally specify a TTL (time to live) and whether this is a sliding or fixed period.

E.g. Within Parts Unlimited the product category has a TTL of 10 minutes. 
After 10 minutes, Redis will evict this product category on our behalf. The next user who makes the same request will cause a cache miss. Based on the 'Cache-Aside pattern', this will trigger a load from the source system to load the underlying record, push this into Redis, and then return the product category to the application.

One approach to avoid this waiting time and ensure predictable performance is to prime the cache.

> A possible implementation could be a webjob running in the back ground either on a schedule or triggered by Azure storage / Azure queue. See [Create a .NET WebJob](https://azure.microsoft.com/en-us/documentation/articles/websites-dotnet-webjobs-sdk-get-started/)

####Operations / Monitoring####

From within the Azure portal, you can review cache misses vs cache hits, memory and cpu usage and load.
Based on these metrics if you find that your cache is no longer meeting the requirements of your application, you can scale you cache. For more information on monitoring see [How to monitor Azure Redis Cache](https://azure.microsoft.com/en-us/documentation/articles/cache-how-to-monitor/) and for more on [How to scale Azure Redis Cache](https://azure.microsoft.com/en-us/documentation/articles/cache-how-to-scale/)

You can also introduce your own monitoring and scaling operations across the cache.  Example implementations based on the Azure Management Libraries (MAML) are shown [here](https://github.com/rustd/RedisSamples/tree/master/ManageCacheUsingMAML). To alter the Redis SKU using MAML, see ['How to automate a scaling operation'](https://azure.microsoft.com/en-us/documentation/articles/cache-how-to-scale/#how-to-automate-a-scaling-operation).