##DocumentDB; Storage and indexing of Arbitrary Data Structures##

Microsoft Azure DocumentDB is the highly-scalable NoSQL document database-as-a-service that offers rich query and transactions over schema-free data, helps deliver reliable and predictable performance, and enables rapid development.

To highlight the capability of the schema free database we will show an architectural approach to address the schema free and embrace loose typings. This will be shown through storage of the product catalogue with products which contain deeply nested attributes and show an approach how they can be shown and queried across.

We also highlight common mistakes made when implementing and querying DocumentDB and ways in which to avoid or address these.

###Setup###
For this section we assume basic knowledge of setting up and consuming DocumentDB inside your application. A comprehensive guide to getting started with DocumentDB can be found [here](https://azure.microsoft.com/en-us/documentation/articles/documentdb-create-account/)

Modifications to Parts Unlimited for this section will require a DocumentDB database account created in Azure and linked to your application. To house our products, we will create a database inside our DocumentDB Account called "PartsUnlimited", with a Collection called "ProductCollection". The config.json file found inside the root of the PartsUnlimited project contains a "DocDb" section which specifies the DocumentDB URI and Key to use unique to your instance of Document DB.
		
###Managing collections and items###

####Schema free storage approach####

Consider the Parts Unlimited scenario of building an e-commerce site where products can cover a wide range of items with unique and different properties. It soon becomes difficult to present and maintain this type of unstructured data in a tabular format. Creating a column for each possible attribute doesn't scale because there are too many varying attributes among your various products. Alternatively, creating a table for each product type is cumbersome given the potential variety of the product catalog. In addition to this, maintaining and updating columns of various products over time becomes a large maintenance cost and carries significant risk when the application relies on a strictly set schema. Storing products as JSON in a single varchar column has significant drawbacks in regards to performance, and you lose the ability to index and query against individual properties. This is where the use of schema free storage in the form of Azure DocumentDB should be considered. Storing heterogeneous data, or data with an undefined schema with the potential of changing frequently in a schema-agnostic database solves these issues while still providing full indexing, enabling you to easily and efficiently query against your data set.

Naturally, a schema will need to be applied to the data for use with the application at some stage, with common approaches doing so in either the application layer (e.g. deserializng result sets to a typed object), or within the UI layer for presentation to the end user. An alternative option is to store the schema against the item inside DocumentDB, and creating templates in your UI layer that can consume this schema at runtime in order to display data correctly. This makes it possible to still have a defined schema, which can be altered without need for changes to the application layer or UI layer itself, providing the flexibility of storing data in a schema agnostic database while still providing some form of a defined schema for use within your application.

####Collection partitioning strategy####
	
Collections act as highly available data partitions for document storage and processing. A common practice when migrating from a SQL database implementation to DocumentDB is to define an individual DocumentDB collection for each table or item type. It is however important to consider that a collection within DocumentDB can store heterogeneous items with a diverse range of content and not tied to a particular type or schema. Additionally, Azure DocumentDB pricing is based on a "per collection" model, with larger amounts of collections incurring higher costs. It is therefore recommended to have a collection partitioning strategy that minimises the amount of total collections, and to view collections as units of partitions that provide boundaries for transactions and query execution rather than tables, thus having these collections driven by capacity requirements (individual collection have a 10GB capacity constraint), or by throughput requirements for the individual collection based on the data housed within, as individual collections can be assigned different performance tiers (see the throttling section below).

Different "types" of items can be stored within an individual collection by creating a "type" discriminator property for each entry within the collection to enable filtering by item type. e.g.

	SELECT * FROM c WHERE c.type = "Product"

It is worth noting that by default, all properties within DocumentDB are hash-indexed, resulting in a negligible performance hit for filtering for items in this manner. However if you have opted to specify a custom indexing policy, the "type" property should be configured to be indexed in order to assist in performance when filtering by type. More information on DocumentDB indexing policies can be found [here](https://azure.microsoft.com/en-us/documentation/articles/documentdb-indexing-policies/).

From a performance perspective, an added benefit of structuring collections in a manner that collocates heterogeneous documents in a singular collection is that this provides the most efficient read pattern when querying across a set of items, as the initial setup work that includes fetching the physical address of a partition, as well as "warming" the connection pool only has to occur once, providing reduced request latency for subsequent calls to the individual collection.

###Querying DocumentDB###
	
####Complex arbitrary JSON documents####

Azure DocumentDB provides the ability to query documents using a subset of SQL as a schema free JSON query language. This is possible through the automatic indexing of JSON documents, foregoing the need for creating explicit schemas, or creating secondary indexes. This effectively means that all we need to do in order to query complex JSON documents, is to insert the document into a DocumentDB collection, and query the document using the SQL language provided. It is however important to note the differences between dealing with DocumentDB in comparison to a more traditional SQL database. DocumentDb SQL works with JSON, and therefore a tree shaped structure, as opposed to rows and columns, giving us the ability to refer to nodes in the tree by depth eg. Node1.Node2.Node3 etc. Therefore, DocumentDb only works with [supported JSON](http://www.json.org/) formatted documents. While DocumentDb supports hierarchical, relational and spacial queries, along with standard ANSI-SQL keywords and operations, it is also worth noting some SQL equivalent operations are not supported for DocumentDB, with a comprehensive list available [here](https://msdn.microsoft.com/en-us/library/azure/dn782250.aspx).

DocumentDB enables us to perform fast, strongly typed queries on arbritrary data structures. We have the ability to query across deeply nested properties within a collection of JSON documents due to the combination of the fully indexed nature of DocumentDB, as well as the fact that relations in data entities are implicitly captured by containment inside the document as opposed to primary/foreign key relations. This enables us to query these deeply nested, complex JSON documents without the need for specifying a structure or schema to our query in advance. The query return type as a result, is bound dynamically, and therefore the same expression can yield different types when executed on different documents i.e. the result of a query is not gaurenteed to be of a fixed schema.

* Allow users to see a BoM (bill of materials) type structure and to search for similar items deep within that structure where search relies on indexed arbitrary structure
* Show the power of being able to perform a fast, strongly typed query, on arbitrary data structures
* Call out the indexing options and benefits of each vs query patterns
* Querying capability where related data is numeric, or a string, to show that it's not just Full Text Indexing capability, and we want to be deeply nested
* Search for related products with SQL like query
* Expand on RDBMS operations that are not available for DocumentDB, and call out some ways around them (call out its SQL-like)

####Caching query objects####

A common mistake is to create a new DocumentClient each time the application intends to perform a request to DocumentDB. Each DocumentClient instance is thread-safe and performs efficient connection management and address caching when operating in Direct Mode. Therefore, creating a new instance of DocumentClient is an expensive operation that can have performance implications. To allow efficient connection management and better performance by DocumentClient, it is recommended to use a single instance of DocumentClient per AppDomain for the lifetime of the application.

In Azure DocumentDB, each document has a system-generated SelfLink. These SelfLinks are guaranteed to be unique and immutable for the lifetime of the document, and reading a single document using a SelfLink is commonly referenced as the most efficient way to consume a single document. It is important to note however, that a common mistake is to create new instances of 'GetOrCreateDatabase' or 'GetOrCreateCollection' every time a reference to a database or collection is needed in order to retrieve SelfLinks. This results in multiple queries to DocumentDB for every single intended operation, and can result in exceeding your request quota and getting throttled. It is therefore recommended to cache these objects whenever suitable if they are required by the application.

It is currently possible to do away with SelfLinks to a large extent within your application, avoiding this issue entirely. An UriFactory can be used to construct links that are based on the ID property of items, and therefore a query for databases or collections is not needed in many cases. In the event where the application has to ensure that a collection or database exists, 'GetOrCreateDatabase' or 'GetOrCreateCollection' can still be used, but the return objects should then be cached to avoid the issues highlighted above.

**TODO - SB - Expand on / Insert sample of URI builder with ID bases routing **
		
####Managing throttling####

Azure DocumentDB implements a reserved throughput model for each individual collection that resides in your database account. Throughput requirements can be managed for individual collections as requirements from the application change by setting their respective performance levels, which can be viewed [here](https://azure.microsoft.com/en-us/documentation/articles/documentdb-performance-levels/)

As a result of throughput throttling on individual collections, it is possible to observe confusing behaviour from your application as certain requests to DocumentDB succeed and others fail once they have exceeded their individual request quota and get throttled. It is therefore important to account for this behaviour within your application.

When the application exceeds the allowed throughput for the collection, DocumentDB will pre-emptively end the request and respond with HTTP status code 429 (Request too large), and return a "x-ms-retry-after-ms" header representing the amount of time (in milliseconds) that the application must wait before attempting another request:

	HTTP Status 429,
	Status Line: RequestRateTooLarge
	x-ms-retry-after-ms :100
	
When using the DocumentDB .Net SDK with LINQ, the SDK will automatically retry the failed operation internally when it encounters an HTTP 429. There are however scenarios where default throttling exception behaviour from the SDK may not be sufficient, and in such cases the application can be modified to handle the RequestRateTooLargeException. A wrapper allowing for throttling, and retrying a task when the appropriate waiting time has been satisfied can be implemented as follows:

	private static async Task<V> ExecuteTaskWithThrottlingSafety<V>(DocumentClient client, Func<Task<V>> func)
	{
		TimeSpan delayTime = TimeSpan.Zero;

		while (true)
		{
			try
			{
				return await func();
			}
			catch (AggregateException ae) when (ae.InnerException is DocumentClientException)
			{
				DocumentClientException de = (DocumentClientException)ae.InnerException;
				if((int)de.StatusCode == 429)
				{
					delayTime = de.RetryAfter;
				}
				else
				{	
					throw;
				}                   
			}
			
			await Task.Delay(delayTime);
		}
	}	
	
This can then be used to call an operation against DocumentDB while allowing for throttling safety and retry functionality:

	Product newProduct = new Product();
	ResourceResponse<Document> response = await ExecuteTaskWithThrottlingSafety(client, () => client.CreateDocumentAsync(collectionSelfLink, newProduct));

####Result sets as JSON to client####

There is a performance overhead involved with serialising and deserialising JSON documents when documents are retrieved from DocumentDB. It is however currently possible to skip this overhead and pass the JSON document directly to the client. The serialisation process is triggered the moment a property on the retrieved document is accessed e.g.

	var id = document.Id;
	
This can be circumvented by using a JsonWriter on the document object in order to retrieve the string representation of the document. With this method, the deserialization process is skipped. Additionally it is possible to load from the string directly. Resource.LoadFrom is a method available inside the DocumentDB SDK, which enables loading from a specified JSON reader. More information can be found [here](https://msdn.microsoft.com/en-us/library/azure/microsoft.azure.documents.resource.aspx).